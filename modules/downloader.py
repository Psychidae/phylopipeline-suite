import streamlit as st
import pandas as pd
import time
from Bio import Entrez, SeqIO
from io import StringIO, BytesIO
import xml.etree.ElementTree as ET

def app_downloader():
    st.header("ğŸ§¬ GenBank Sequence Downloader")
    st.info("é…åˆ—ãƒ‡ãƒ¼ã‚¿(FASTA)ã¨ã€è©³ç´°ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(Excel)ã‚’ä¸€æ‹¬å–å¾—ã—ã¾ã™ã€‚")

    col1, col2 = st.columns([1, 1.5])
    with col1:
        st.subheader("è¨­å®š")
        email = st.text_input("Email (å¿…é ˆ)", placeholder="your_email@example.com", help="NCBIã®åˆ©ç”¨è¦ç´„ã«ã‚ˆã‚Šå¿…é ˆã§ã™")
        target_gene = st.text_input("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆéºä¼å­", value="COI", help="ä¾‹: COI, 16S, NADH dehydrogenase subunit 1")
        max_ret = st.number_input("1ç¨®ã‚ãŸã‚Šã®æœ€å¤§å–å¾—æ•°", 1, 100, 1)
        
        st.markdown("---")
        st.caption("çµã‚Šè¾¼ã¿ã‚ªãƒ—ã‚·ãƒ§ãƒ³ (ä»»æ„)")
        filter_author = st.text_input("ç™»éŒ²è€…/è‘—è€…å", placeholder="Smith J", help="ã“ã®è‘—è€…ãŒé–¢ã‚ã£ãŸé…åˆ—ã®ã¿å–å¾—")
        filter_journal = st.text_input("è«–æ–‡/é›‘èªŒå", placeholder="Nature", help="ã“ã®é›‘èªŒ/è«–æ–‡ã«å«ã¾ã‚Œã‚‹é…åˆ—ã®ã¿å–å¾—")

    with col2:
        st.subheader("ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        template_csv = "Homo sapiens\nMus musculus\nDrosophila melanogaster\n"
        st.download_button(
            "ğŸ“¥ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆCSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            template_csv,
            "species_list_template.csv",
            "text/csv",
            help="ç¨®åã‚’1è¡Œã«1ã¤è¨˜è¼‰ã—ãŸCSV/TXTãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¦ãã ã•ã„"
        )
        
        uploaded_file = st.file_uploader("ç¨®åãƒªã‚¹ãƒˆ (CSV/TXT, ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—)", type=["csv", "txt"])
        
        if uploaded_file and email:
            if st.button("ğŸš€ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹", type="primary"):
                try:
                    df_input = pd.read_csv(uploaded_file, header=None)
                    species_list = df_input[0].tolist()
                    Entrez.email = email
                    
                    fasta_records = []
                    metadata_list = []
                    
                    log_text = ""
                    prog_bar = st.progress(0)
                    status_text = st.empty()
                    
                    for i, sp in enumerate(species_list):
                        prog_bar.progress((i + 1) / len(species_list))
                        status_text.text(f"Searching: {sp}...")
                        
                        # æ¤œç´¢ã‚¯ã‚¨ãƒªæ§‹ç¯‰
                        term = f'"{sp}"[Organism] AND {target_gene}[All Fields]'
                        if filter_author:
                            term += f' AND {filter_author}[Author]'
                        if filter_journal:
                            term += f' AND {filter_journal}[Journal]'
                            
                        try:
                            # 1. IDæ¤œç´¢
                            handle = Entrez.esearch(db="nucleotide", term=term, retmax=max_ret)
                            record = Entrez.read(handle)
                            id_list = record["IdList"]
                            
                            if not id_list:
                                log_text += f"âŒ {sp}: ãªã— (æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“)\n"
                                continue
                            
                            # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾— (GBå½¢å¼XML)
                            # FASTAã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆ¥ã€…ã«å–ã‚‹ã®ã¯åŠ¹ç‡ãŒæ‚ªã„ã®ã§ã€GBãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ã—ã¦ãƒ‘ãƒ¼ã‚¹ã™ã‚‹
                            handle_gb = Entrez.efetch(db="nucleotide", id=id_list, rettype="gb", retmode="xml")
                            gb_records = Entrez.parse(handle_gb)
                            
                            count = 0
                            for rec in gb_records:
                                # åŸºæœ¬æƒ…å ±
                                accession = rec.get("GBSeq_primary-accession", "")
                                definition = rec.get("GBSeq_definition", "")
                                length = rec.get("GBSeq_length", "")
                                sequence = rec.get("GBSeq_sequence", "").upper()
                                create_date = rec.get("GBSeq_create-date", "")
                                update_date = rec.get("GBSeq_update-date", "")
                                
                                # è«–æ–‡æƒ…å ± (æœ€åˆã®Reference)
                                refs = rec.get("GBSeq_references", [])
                                journal = ""
                                authors = ""
                                title = ""
                                if refs:
                                    first_ref = refs[0]
                                    journal = first_ref.get("GBReference_journal", "")
                                    title = first_ref.get("GBReference_title", "")
                                    auth_list = first_ref.get("GBReference_authors", [])
                                    authors = ", ".join(auth_list) if auth_list else ""

                                # ç‰¹å¾´ãƒ†ãƒ¼ãƒ–ãƒ« (Sourceã‹ã‚‰æ¡å–åœ°æƒ…å ±ã‚’æ¢ã™)
                                country = ""
                                lat_lon = ""
                                collection_date = ""
                                collector = ""
                                isolation_source = ""
                                geo_loc_name = ""
                                
                                features = rec.get("GBSeq_feature-table", [])
                                for feat in features:
                                    if feat["GBFeature_key"] == "source":
                                        quals = feat.get("GBFeature_quals", [])
                                        for q in quals:
                                            if q["GBQualifier_name"] == "country": country = q["GBQualifier_value"]
                                            if q["GBQualifier_name"] == "lat_lon": lat_lon = q["GBQualifier_value"]
                                            if q["GBQualifier_name"] == "collection_date": collection_date = q["GBQualifier_value"]
                                            if q["GBQualifier_name"] == "collected_by": collector = q["GBQualifier_value"]
                                            if q["GBQualifier_name"] == "isolation_source": isolation_source = q["GBQualifier_value"]
                                            if q["GBQualifier_name"] == "geo_loc_name": geo_loc_name = q["GBQualifier_value"]

                                # FASTAç”¨ãƒ¬ã‚³ãƒ¼ãƒ‰ä½œæˆ
                                clean_sp = sp.replace(" ", "_")
                                seq_id = f"{clean_sp}_{accession}"
                                fasta_records.append(f">{seq_id} {definition}\n{sequence}\n")
                                
                                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆè¿½åŠ 
                                metadata_list.append({
                                    "Sequence_ID": seq_id,
                                    "Species": sp,
                                    "Accession": accession,
                                    "Definition": definition,
                                    "Length": length,
                                    "Country": country,
                                    "Geo_Loc_Name": geo_loc_name,
                                    "Isolation_Source": isolation_source,
                                    "Lat_Lon": lat_lon,
                                    "Collection_Date": collection_date,
                                    "Collector": collector,
                                    "Authors": authors,
                                    "Journal": journal,
                                    "Title": title,
                                    "Create_Date": create_date,
                                    "Update_Date": update_date
                                })
                                count += 1
                                
                            log_text += f"âœ… {sp}: {count}ä»¶å–å¾—\n"
                            time.sleep(0.5) # APIåˆ¶é™å›é¿
                            
                        except Exception as e:
                            log_text += f"âš ï¸ {sp}: Error {e}\n"
                    
                    status_text.success("å®Œäº†ï¼")
                    st.text_area("ãƒ­ã‚°", log_text, height=100)
                    
                    if fasta_records:
                        # FASTAãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                        fasta_str = "".join(fasta_records)
                        st.download_button("ğŸ“¥ FASTAã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", fasta_str, "sequences.fasta")
                        
                        # Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                        if metadata_list:
                            df_meta = pd.DataFrame(metadata_list)
                            excel_buffer = BytesIO()
                            with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                                df_meta.to_excel(writer, index=False, sheet_name='Metadata')
                            
                            st.download_button(
                                "ğŸ“¥ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(Excel)ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                excel_buffer.getvalue(),
                                "metadata.xlsx",
                                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                            )
                    else:
                        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚çµã‚Šè¾¼ã¿æ¡ä»¶ãŒå³ã—ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                        
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        elif uploaded_file:
            st.warning("ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
